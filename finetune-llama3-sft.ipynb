{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune HF Llama 3.0 and Deploy on AWS Bedrock\n",
    "\n",
    "This notebook has the following steps: \n",
    "\n",
    "1. imports and converts [Llama 3.0 8b](https://huggingface.co/meta-llama/Meta-Llama-3-8B) from Hugging Face transformer file format to .nemo file format\n",
    "\n",
    "    Note: you will need to create a HuggingFace account and request access to the model\n",
    "\n",
    "2. Supervised Fine Tuning (SFT) using the NeMo framework on the [NVIDIA Daring-Anteater dataset](https://huggingface.co/datasets/nvidia/Daring-Anteater), a comprehensive dataset for instruction tuning\n",
    "\n",
    "3. Move your finetuned model to AWS S3 for use with AWS Bedrock Custom Model Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Hugging Face Model to NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import huggingface_hub\n",
    "\n",
    "# Set your Hugging Face access token\n",
    "huggingface_hub.login(\"HUGGING_FACE_TOKEN\")\n",
    "os.makedirs(\"/demo-workspace/Meta-Llama-3-8B\" ,exist_ok=True)\n",
    "huggingface_hub.snapshot_download(repo_id=\"meta-llama/Meta-Llama-3-8B\", repo_type=\"model\", local_dir=\"Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# clear any previous temporary weights dir if any\n",
    "rm -r model_weights\n",
    "\n",
    "#converter script from NeMo\n",
    "python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "  --precision bf16 \\\n",
    "  --input_name_or_path=/demo-workspace/Meta-Llama-3-8B \\\n",
    "  --output_path=/demo-workspace/Meta-Llama-3-8B.nemo \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Configure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "dataset = load_dataset(\"nvidia/daring-anteater\")\n",
    "\n",
    "for split, shard in dataset.items():\n",
    "    length = len(shard)\n",
    "    train_limit = length * 0.85\n",
    "    with open(\"daring-anteater-train.jsonl\", \"w\") as train:\n",
    "        with open(\"daring-anteater-val.jsonl\", \"w\") as val:\n",
    "            for count, line in enumerate(shard):\n",
    "                desired_data = {\n",
    "                    \"system\": line[\"system\"],\n",
    "                    \"conversations\": line[\"conversations\"],\n",
    "                    \"mask\": line[\"mask\"],\n",
    "                    \"type\": \"TEXT_TO_VALUE\",\n",
    "                }\n",
    "                if count < train_limit:\n",
    "                    json.dump(desired_data, train)\n",
    "                    train.write('\\n')\n",
    "                else:\n",
    "                    json.dump(desired_data, val)\n",
    "                    val.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Set paths to the model, train, validation and test sets.\n",
    "MODEL=\"/demo-workspace/Meta-Llama-3-8B.nemo\"\n",
    "TRAIN_DS=\"./daring-anteater-train.jsonl\"\n",
    "VALID_DS=\"./daring-anteater-val.jsonl\"\n",
    "TEST_DS=\"./daring-anteater-val.jsonl\"\n",
    "TEST_NAMES=\"[daring-anteater]\"\n",
    "\n",
    "SCHEME=\"none\"  # SFT is none\n",
    "TP_SIZE=2\n",
    "PP_SIZE=1\n",
    "\n",
    "OUTPUT_DIR=\"/demo-workspace/llama3-8b-daring-anteater-sft-3\"\n",
    "\n",
    "export HYDRA_FULL_ERROR=1\n",
    "\n",
    "python /opt/NeMo-Aligner/examples/nlp/gpt/train_gpt_sft.py \\\n",
    "   trainer.precision=bf16 \\\n",
    "   trainer.num_nodes=1 \\\n",
    "   trainer.devices=8 \\\n",
    "   trainer.sft.max_steps=-1 \\\n",
    "   trainer.sft.limit_val_batches=40 \\\n",
    "   trainer.sft.val_check_interval=1000 \\\n",
    "   model.megatron_amp_O2=True \\\n",
    "   model.restore_from_path=${MODEL} \\\n",
    "   model.optim.lr=5e-6 \\\n",
    "   model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "   model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "   model.data.chat=True \\\n",
    "   model.data.num_workers=0 \\\n",
    "   model.data.train_ds.micro_batch_size=1 \\\n",
    "   model.data.train_ds.global_batch_size=4 \\\n",
    "   model.data.train_ds.max_seq_length=8192 \\\n",
    "   model.data.train_ds.file_path=${TRAIN_DS} \\\n",
    "   model.data.validation_ds.micro_batch_size=1 \\\n",
    "   model.data.validation_ds.global_batch_size=4 \\\n",
    "   model.data.validation_ds.file_path=${VALID_DS} \\\n",
    "   model.data.validation_ds.max_seq_length=8192 \\\n",
    "   exp_manager.create_wandb_logger=False \\\n",
    "   exp_manager.explicit_log_dir=${OUTPUT_DIR} \\\n",
    "   exp_manager.wandb_logger_kwargs.project=llama3-8b-sft \\\n",
    "   exp_manager.wandb_logger_kwargs.name=chat_sft_run \\\n",
    "   exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "   exp_manager.resume_if_exists=False \\\n",
    "   exp_manager.resume_ignore_no_checkpoint=True \\\n",
    "   exp_manager.create_checkpoint_callback=True \\\n",
    "   exp_manager.checkpoint_callback_params.monitor=val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model to AWS S3\n",
    "\n",
    "To prepare the model for use with BedRock, we must first convert our finetuned model weights back to HF safetensors. The model and the original llama 3.0 tokens will then be sent to your S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python /opt/NeMo/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py \\\n",
    "--input_name_or_path /demo-workspace/llama3-8b-daring-anteater-sft-3/checkpoints/megatron_gpt_sft.nemo \\\n",
    "--output_path /demo-workspace/llama-output-weights.bin \\\n",
    "--hf_input_path /demo-workspace/Meta-Llama-3-8B \\\n",
    "--hf_output_path /demo-workspace/sft-llama-3-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=<INSERT_ACCESS_KEY_ID>\n",
    "export AWS_SECRET_ACCESS_KEY=<INSERT_SECRET_ACCESS_KEY>\n",
    "\n",
    "./s5cmd cp /demo-workspace/sft-llama-3-hf s3://<INSERT_BUCKET_NAME>\n",
    "\n",
    "./s5cmd cp /demo-workspace/Meta-Llama-3.0-8B/tokenizer.json s3://<INSERT_BUCKET_NAME>/sft-llama-3-hf/\n",
    "./s5cmd cp /demo-workspace/Meta-Llama-3.0-8B/tokenizer_config.json s3://<INSERT_BUCKET_NAME>/sft-llama-3-hf/\n",
    "./s5cmd cp /demo-workspace/Meta-Llama-3.0-8B/original/tokenizer.model s3://<INSERT_BUCKET_NAME>/sft-llama-3-hf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run with BedRock, go to the Custom Model import feature and load your model from your S3 bucket. Once the model is ready, it can directly be used for your production inference. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
